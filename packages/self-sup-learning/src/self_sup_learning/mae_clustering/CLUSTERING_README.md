# Clustering Analysis on MAE ViT Embeddings

This directory contains scripts for performing clustering analysis on the ImageNet embeddings generated by the MAE ViT model.

## Overview

Three clustering methods are implemented:
1. **k-means**: Classic partitional clustering
2. **VBGMM**: Variational Bayesian Gaussian Mixture Model (soft clustering with automatic component selection)
3. **HDBSCAN**: Hierarchical Density-Based Spatial Clustering (finds arbitrary-shaped clusters and noise)

## Quick Start

### 1. Generate Embeddings (if not done yet)
```bash
python playground/eval_pretrained_models.py
```

### 2. Test Clustering Pipeline
```bash
python playground/quick_clustering_test.py
```
This runs a quick test on 5,000 samples to verify everything works.

### 3. Run Full Clustering Analysis
```bash
python playground/clustering_analysis.py
```

## Configuration

The clustering configuration is managed by Hydra in `configs/clustering_config.yaml`. You can:

### Modify Configuration File
Edit `configs/clustering_config.yaml` to change parameters.

### Override from Command Line
```bash
# Run only k-means
python playground/clustering_analysis.py methods.vbgmm.enabled=false methods.hdbscan.enabled=false

# Try specific k values for k-means
python playground/clustering_analysis.py methods.kmeans.n_clusters=[100,500,1000]

# Change output directory
python playground/clustering_analysis.py output.results_dir=./my_results

# Disable visualization
python playground/clustering_analysis.py visualization.enabled=false
```

## Clustering Methods

### k-means
**Parameters:**
- `n_clusters`: Number of clusters to find (default: [10, 50, 100, 200, 500, 1000])
- `max_iter`: Maximum iterations (default: 300)
- `n_init`: Number of random initializations (default: 10)

**Best for:** Well-separated, spherical clusters with known number of clusters

### VBGMM (Variational Bayesian Gaussian Mixture Model)
**Parameters:**
- `n_components`: Maximum number of components (default: [50, 100, 200, 500, 1000])
- `covariance_type`: Covariance matrix type (default: "diag")
  - "full": Each component has its own covariance matrix
  - "tied": All components share the same covariance matrix
  - "diag": Diagonal covariance (faster, assumes features are independent)
  - "spherical": Single variance per component
- `weight_concentration_prior`: Controls component pruning (default: 0.001)

**Best for:** Soft clustering, when you want to automatically determine the optimal number of clusters (will use fewer than n_components if appropriate)

**Note:** VBGMM may collapse to fewer clusters than specified, especially on high-dimensional data. This is expected behavior.

### HDBSCAN (Hierarchical Density-Based Clustering)
**Parameters:**
- `min_cluster_size`: Minimum number of samples in a cluster (default: [100, 200, 500])
- `min_samples`: How conservative clustering should be (default: [10, 20, 50])
- `cluster_selection_method`: "eom" (more clusters) or "leaf" (fewer clusters)

**Best for:** Finding arbitrary-shaped clusters, handling noise, unknown number of clusters

**Note:** HDBSCAN assigns noise points to cluster label -1. These are excluded from most metrics.

## Output

All results are saved to `./clustering_results/` (configurable):

### Prediction Files
- `{method}_{params}_predictions.csv`: Cluster assignments for each image
  - Columns: filename, class_id, class, path, cluster_label

### Visualizations
- `{method}_{params}_visualization.png`: 2D visualization using UMAP/t-SNE/PCA
  - Points colored by cluster
  - Noise points shown in gray (for HDBSCAN)

### Summary Files
- `clustering_summary.csv`: All results in tabular format
- `clustering_summary.json`: Detailed results in JSON format

## Evaluation Metrics

### Internal Metrics (no ground truth needed)
- **Silhouette Score** [-1, 1]: Higher is better. Measures cluster cohesion vs separation
- **Calinski-Harabasz Score** [0, ∞): Higher is better. Ratio of between-cluster to within-cluster variance
- **Davies-Bouldin Score** [0, ∞): Lower is better. Average similarity between clusters

### External Metrics (uses ImageNet labels)
- **Adjusted Rand Index** [-1, 1]: Similarity to true labels (1 = perfect, 0 = random)
- **Normalized Mutual Information** [0, 1]: Information shared between clusterings and true labels
- **V-Measure** [0, 1]: Harmonic mean of homogeneity and completeness

### Additional Info
- **n_clusters**: Number of clusters found
- **n_noise_points**: Number of noise points (HDBSCAN only)
- **noise_ratio**: Proportion of noise points

## Example Results

```
method  n_clusters_param  n_clusters  silhouette  calinski_harabasz  davies_bouldin  adjusted_rand_index
kmeans              100         100       0.125            1234.56            1.85                 0.45
kmeans              500         500       0.098            2345.67            2.12                 0.52
vbgmm               100          87       0.108            1567.89            1.93                 0.41
hdbscan             N/A          45       0.152             987.65            1.67                 0.38
```

## Tips for High-Dimensional Embeddings

1. **Start with k-means**: It's fast and provides a good baseline
2. **Use larger min_cluster_size for HDBSCAN**: High-dim data needs more samples to reliably estimate density
3. **VBGMM covariance**: Use "diag" for speed, "full" for accuracy (but slower)
4. **Visualization**: UMAP is generally better than t-SNE for preserving global structure

## Computational Considerations

### Time Complexity (approximate, for N=50,000 samples, D=768 dimensions)
- **k-means (k=100)**: ~30 seconds
- **k-means (k=1000)**: ~5 minutes
- **VBGMM (n=100)**: ~2 minutes
- **VBGMM (n=1000)**: ~20 minutes
- **HDBSCAN**: ~10-30 minutes (depends on parameters)

### Memory Usage
- All methods: ~1-2 GB for 50,000 samples
- VBGMM with covariance_type="full": Can be higher for large n_components

### Recommendations
- For quick exploration: Run k-means with a few k values
- For thorough analysis: Run all methods but be prepared to wait
- Use `visualization.max_samples_plot` to limit visualization time

## Troubleshooting

### VBGMM finds only 1 cluster
- This is normal for VBGMM with high-dim data and small n_components
- Try increasing n_components or using k-means instead
- Adjust weight_concentration_prior

### HDBSCAN finds no clusters (all noise)
- Increase min_cluster_size and/or decrease min_samples
- High-dimensional data is sparse; HDBSCAN may struggle
- Consider dimensionality reduction (PCA) before clustering

### Out of memory
- Reduce batch size or subsample data
- Use VBGMM with covariance_type="diag" instead of "full"
- Process methods one at a time by disabling others in config

### Slow visualization
- Reduce visualization.max_samples_plot
- Use PCA instead of UMAP/t-SNE
- Disable visualization entirely

## Advanced Usage

### Custom Parameter Grid Search
```python
# Create a custom config file
# configs/my_clustering.yaml
defaults:
  - clustering_config

methods:
  kmeans:
    n_clusters: [100, 200, 300, 400, 500]
```

```bash
python playground/clustering_analysis.py --config-name=my_clustering
```

### Programmatic Usage
```python
from clustering_analysis import run_kmeans, evaluate_clustering, load_embeddings_from_lancedb

# Load data
embeddings, metadata = load_embeddings_from_lancedb("./lancedb", "imagenet_mae_embeddings")

# Run clustering
labels, model = run_kmeans(embeddings, n_clusters=100)

# Evaluate
metrics = evaluate_clustering(embeddings, labels, true_labels=metadata['class_id'].values)
print(metrics)
```

## References

- **k-means**: MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations"
- **VBGMM**: Blei, D. M., & Jordan, M. I. (2006). "Variational inference for Dirichlet process mixtures"
- **HDBSCAN**: McInnes, L., Healy, J., & Astels, S. (2017). "hdbscan: Hierarchical density based clustering"

## Dependencies

- scikit-learn: k-means, VBGMM, evaluation metrics
- hdbscan: HDBSCAN clustering
- umap-learn: UMAP dimensionality reduction for visualization
- matplotlib, seaborn: Plotting
- hydra-core: Configuration management
- lancedb: Embedding database
